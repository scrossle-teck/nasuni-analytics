# WP10 Addendum — Quality, performance, and operations

## What you can leverage today

### Completed work in `nasuni-inventory`

- Strong PowerShell quality discipline:
  - strict mode and careful error handling
  - Pester tests with isolated shims and fixtures
  - analyzer hygiene (PSScriptAnalyzer settings)
  - explicit exit codes suitable for CI

Key references (inventory repo):

- `pester.ps1`
- `Tests/*`
- `PSScriptAnalyzerSettings.psd1`

### Completed work in `nasuni-analytics`

- Python test scaffolding exists (`pytest`), and some core ingest/rule logic is tested.
- Static typing tooling is present (`pyrightconfig.json`, optional pyright in `run.py test`).
- There is a scalability metrics script already.

Key references:

- `tests/test_ingest.py`
- `tests/test_apply_rules.py`
- `tests/test_collate_scalability_metrics.py`
- `scripts/collate_scalability_metrics.py`
- `run.py` (test runner)

### Known current limitation to plan around

- Several analytics scripts load all Parquet into pandas (`pd.concat`) before processing.
  - This is the main scaling risk and should be addressed as part of WP04 (SQL-first), then validated here.

## Gaps vs WP10 acceptance criteria

WP10 expects:

- fixtures and regression tests for the high-risk pieces (fingerprints, closure, effective access)
- performance baselines and repeatable runs
- CI discipline and clear operational docs

Current gaps:

- No fixture coverage yet for:
  - WP03 canonicalization + ACL-definition fingerprints
  - WP06 membership closure (cycles)
  - WP08 effective access
  - WP09 run-to-run diffs
- No explicit performance baselines recorded and tracked per run.

## Checklist: steps to reach WP10 acceptance

### Expand fixture tests

- [ ] Add synthetic datasets for:
  - [ ] canonicalization + fingerprint determinism
  - [ ] nested groups + cycles closure
  - [ ] effective access “write-like” query
  - [ ] run-to-run diff scenarios

### Regression strategy

- [ ] Ensure “same input snapshot + same ruleset ⇒ same output” tests exist.
- [ ] Track ruleset and canonicalization versions in outputs.

### Performance baselines

- [ ] Define baseline metrics:
  - [ ] ingest time
  - [ ] rule run time
  - [ ] closure compute time
  - [ ] effective query time
- [ ] Record them (manifest is ideal; see WP01).

### Operational docs

- [ ] Document an end-to-end run recipe:
  - [ ] inventory run → analytics ingest → findings
  - [ ] include troubleshooting for missing columns/schema variance

### CI

- [ ] Ensure CI runs:
  - [ ] `pytest`
  - [ ] `pyright` (when available)
- [ ] Consider a small “smoke dataset” included in the repo (or generated by tests) to keep CI fast.

## Notes

- Reuse `nasuni-inventory`’s testing approach as a model: isolated shims, explicit fixtures, deterministic outputs.
- WP10 should be treated as “ongoing”, but the acceptance criteria can be satisfied incrementally as each WP lands.
